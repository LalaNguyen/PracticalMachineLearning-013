---
title: "Project-WriteUp"
author: "Lala Ng"
date: "April 22, 2015"
output: html_document
---


### I. Cleaning & Prepocessing Data

In this assignment, we will require **caret** and **randomForest** library to workout the solution. In addition, preprocessing the raw file is trivial to remove noise data

```{r, results='asis',echo=FALSE, message=FALSE,warning=FALSE}
require(ggplot2)
require(caret)
require(rattle)

```

```{r,results='markup',message=FALSE,warning=FALSE}

# Load up file and preprocessing
train <- read.csv(file = destTrainFile, header = TRUE, sep = ",", na.strings = c("NA","#DIV/0!", ""), stringsAsFactors = FALSE)
test <- read.csv(file = destTestFile, header = TRUE, sep = ",", na.strings = c("NA","#DIV/0!", ""), stringsAsFactors = FALSE)

# Delete columns with all missing values
# 1. We find col that has total number of NA equals 0
train_select_col <- colSums(is.na(train)) == 0
test_select_col <- colSums(is.na(test)) == 0

# 2. Filter selected column.
train <- train[,train_select_col]
test <- test[,test_select_col]
train$classe <- as.factor(train$classe)

# 3. user_name, raw_timestamp_part_1, raw_timestamp_part_3
# cvtd_timestamp, new_window, num_window are not particularly # useful.
# Remove it
train <- train[,-c(1:7)]
test <- test[,-c(1:7)]
```

Data is now clean and ready to be extracted. It's often useful to take a look at initial data for me. From the histogram, **class A** frequently occurs while **class D** has the least number of occurences.

```{r,results='asis',message=FALSE,warning=FALSE}
ggplot(train, aes( x = classe)) +
        geom_histogram( aes(fill=..count..)) 
```

### II. Trainning and Testing

We are given **train set** and **test set**, predicting a **class** in test set would require a prediction model to be built first. In this task, we will build up model by using random forest for classification provided from packet **randomForest** on our **classe feature** .

Besides, we will now perform cross validation on our training data. At first, we will split our sample data with the proposion of 75:25 for training:testing. Afterwards, confusion matrix is constructed by comparing the accuracy of the model's prediction with labeled data in sample training data.


```{r,results='markup',message=FALSE,warning=FALSE, echo = FALSE}
# Devide our sample data into train and test to perform
# cross validation
set.seed(2)
inTrain <- createDataPartition(train$classe, p = 0.75, list = FALSE)
sampleTrain <- train[inTrain,]
sampleTest <- train[-inTrain,]

# Train our model on sampleTrain 
modFit <- train(classe ~., method ="rf", data = sampleTrain, prox = FALSE)

```

We achieve very high accuracy by using random forest algorithm. Our in-sample error rate is **0.65%**.

**Warning: Random Forest takes approximately 3 hours on my computer to finish computing**

```{r,results='markup',message=FALSE,warning=FALSE,echo=FALSE}

# Undoubtedly, out-of-sample error of training dataset is also low
sampleTest_preds <-predict(modFit, newdata = sampleTest)
# The accuracy of our model is remarkbly low using rpart...
test_tab <-table(sampleTest_preds, sampleTest$classe)
print("Test Sample of Training Data")
confusionMatrix(test_tab)[3]$overall[1]
```

In addition, the out-of-sample err is also observed at the rate of approximately **0.65%**. We can then make adequate prediction on our 20 test cases.


```{r,results='markup',message=FALSE,warning=FALSE}
# Predict new data
preds <- predict(modFit, newdata = test)
# Filter and evaluate our hypothesis
print("Prediction on 20 cases")
preds
```

Finally, here is my session for library information.

```{r,results='markup', warning=FALSE}
sessionInfo()
```

